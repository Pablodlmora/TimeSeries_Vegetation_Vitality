{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablodlmora\u001b[0m (\u001b[33mvegetation-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import math\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pocketbfast import PocketBfast\n",
                "from datetime import datetime\n",
                "import darts.timeseries as tseries\n",
                "import dask.array as da\n",
                "from nbeats_pytorch.model import NBeatsNet\n",
                "import scipy.stats as stats\n",
                "from pytorch_forecasting import  Baseline, NBeats, TimeSeriesDataSet\n",
                "import torch\n",
                "from torch import nn\n",
                "import lightning.pytorch as pl\n",
                "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
                "from typing import Dict\n",
                "from pytorch_forecasting.models import BaseModel\n",
                "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
                "from pytorch_forecasting.data import NaNLabelEncoder\n",
                "from pytorch_forecasting.data.examples import generate_ar_data\n",
                "from pytorch_forecasting.metrics import SMAPE, MASE, RMSE, MAE\n",
                "import random\n",
                "import itertools\n",
                "from tqdm import tqdm\n",
                "from lightning.pytorch.tuner import Tuner\n",
                "from sklearn.model_selection import ParameterGrid\n",
                "import sys\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import nbeats_training_script as nbt\n",
                "import wandb\n",
                "from pytorch_lightning.loggers import WandbLogger\n",
                "# %env \"WANDB_NOTEBOOK_NAME\" \"Nbeats Wandb Forecasting\"\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "wandb.login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Import the Time Series using the time collector\n",
                "For this Section, it is important to Develop the creation of image time series, cleaned, masked, and set as dask arrays in order to make the script work. It was previously made relying on the proprietary libraries of LiveEO."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-13 11:13:11.990 | INFO     | liveeo_utils.database_utils.postgresql_db_utils:retrieve_geodf_for_sql_query:212 - Running query:\n",
                        "\tSELECT * FROM veg_project_tiles.vitality_ger_kalman_borenkaefer_testing_train_tiles\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Assembler object with root at /efs/pablo/time_experiments/borenkaefer/4b"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dates = [\n",
                "     datetime.strptime(date, \"%Y-%m-%d\") for date in time_series.date_config[\"planet_PS\"][::4]\n",
                " ]\n",
                "gdf_labels = import_vector(file_path=labels_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Splitting Tiles\n",
                "\n",
                "These functions split the tiles in the AOI first into training and testing, and then split the training set of tiles into training and validation. The final sets purposefully leave out the tile 32_N1139_W11 to later test and use for exporting rasters. The random seed in both of the initial functions is optional and was done to get the same split in multiple runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def random_generator_iterator(seq, n, m):\n",
                "    rand_seq = seq[:]\n",
                "    random.seed(8)\n",
                "    random.shuffle(rand_seq)\n",
                "    limit = n-1\n",
                "    for i,group in enumerate(zip(*([iter(rand_seq)]*m))):\n",
                "        yield group\n",
                "        if i == limit: break  \n",
                "\n",
                "\n",
                "\n",
                "def chunks(lst, n):\n",
                "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
                "    m = int((len(lst)/n))\n",
                "    sets = []\n",
                "    for i in range(0, len(lst), m):\n",
                "        sets.append(lst[i:i + m])\n",
                "    np.random.seed(10)\n",
                "    val_index = np.random.choice(10,2)\n",
                "    val_set = np.concatenate((sets[val_index[0]],sets[val_index[1]]),axis=0)\n",
                "    train_set = []\n",
                "    for i in range(len(sets)):\n",
                "        if i not in val_index:\n",
                "            train_set.append(sets[i])\n",
                "    training_set = np.vstack(train_set)\n",
                "    # print(len(sets))\n",
                "    # print('----')\n",
                "    return val_set, training_set     \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tile 32_N1134_W18 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1134_W19 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1139_W13 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1139_W14 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1140_W11 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1140_W13 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1141_W12 stored at /efs/pablo/time_experiments/borenkaefer/4b\n",
                        "Tile 32_N1141_W14 stored at /efs/pablo/time_experiments/borenkaefer/4b\n"
                    ]
                }
            ],
            "source": [
                "def tile_train_test_split(time_series):\n",
                "    training_tiles, testing_tiles = random_generator_iterator(time_series.tiles, 2, 8)\n",
                "    validation_set = []\n",
                "    training_set = []\n",
                "    testing_set = []\n",
                "    testing_labels = []\n",
                "    for tile in time_series:\n",
                "        if tile.name in training_tiles:\n",
                "            print(tile)\n",
                "            tile_values = tile.values.compute()\n",
                "            tile_labels = tile.labels.compute()\n",
                "            \n",
                "            val_set, train_set = chunks(tile_values,10)\n",
                "            validation_set.append(val_set)\n",
                "            training_set.append(train_set)\n",
                "        \n",
                "        else:\n",
                "            testing_tile_values  = tile.values.compute()\n",
                "            testing_tile_labels = tile.labels.compute()\n",
                "            testing_set.append(testing_tile_values)\n",
                "            testing_labels.append(testing_tile_labels)\n",
                "\n",
                "    validation_set = np.vstack(validation_set)\n",
                "    training_set = np.vstack(training_set)\n",
                "    testing_set = np.vstack(testing_set)\n",
                "    # testing_labels = np.vstack(testing_labels)\n",
                "\n",
                "\n",
                "    return validation_set,training_set,testing_set\n",
                "\n",
                "validation_set,training_set,testing_set = tile_train_test_split(time_series)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Functions for the preprocessing of the data, to best work with it for NBEATS, and the Kalman filter\n",
                "\n",
                "* Includes functions to average the time series to have a monthly cadence instead of weekly, and may generate errors if it is not the case. Also has interpolation to handle NaNs.\n",
                "\n",
                "* Presets for a lot of future variables, which shouls be changed according to the shape of the dataset and the length of the time series. Forecast and Backcast lengths are important, and the backcast length always has to be longer than the forecast. Best is to have them do a 50/50 split, or as close to it as possible.\n",
                "\n",
                "* Dictionaries for Hyperparameter tuning with Weights and Biases Library, as well as the set up for the Sweep"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def map_indices(dates):\n",
                "    start = dates[0]\n",
                "    end = dates[-1]\n",
                "    start = datetime(start.year, 1, 1)\n",
                "    end = datetime(end.year, 12, 31)\n",
                "\n",
                "    drange = pd.date_range(start, end, freq=\"d\")\n",
                "    ts = pd.Series(np.ones(len(dates)), dates)\n",
                "    ts = ts.reindex(drange)\n",
                "    indices = np.argwhere(~np.isnan(ts).to_numpy()).T[0]\n",
                "\n",
                "    return indices\n",
                "def average_time_series(array: np.ndarray, window_size: int = 4, mode: str = \"mean\"):\n",
                "    \"\"\"Aggregate the time step values by averaging them.\"\"\"\n",
                "\n",
                "    try:\n",
                "        array = array.reshape(array.shape[0], array.shape[1], -1, window_size)\n",
                "    except IndexError as exception:\n",
                "        raise ValueError(\n",
                "            f\"The input time series length ({array.shape[1]}) can't be divided by\"\n",
                "            f\" the window size ({window_size}). This can be worked around specifying\"\n",
                "            \" a different number for the `from_time_step` parameter to make the\"\n",
                "            \" division fit.\"\n",
                "        ) from exception\n",
                "\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
                "        reducer = da.nanmedian if mode == \"median\" else da.nanmean\n",
                "        array = reducer(array, axis=3)\n",
                "    \n",
                "    return array\n",
                "\n",
                "def interpolate_band(band: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Interpolate missing values in series.\"\"\"\n",
                "    band = band.copy()\n",
                "    is_nan = np.isnan(band)\n",
                "    nan_index = is_nan.nonzero()[0]\n",
                "    value_index = (~is_nan).nonzero()[0]\n",
                "    band[is_nan] = np.interp(nan_index, value_index, band[value_index])\n",
                "    return band"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_count = 16000   # Number of pixels to use.\n",
                "validation_sample_count = 4000\n",
                "data_scale = 10000  # Used to scale the values to [0, 1]\n",
                "nodata_value = 32767\n",
                "fill_value = 0  # Nodata points will be filled with this value.\n",
                "\n",
                "# Data split config.\n",
                "val_size = 270000\n",
                "test_size = 135000\n",
                "\n",
                "# Monitoring period.\n",
                "forecast_length = 30\n",
                "backcast_length = 30  \n",
                "\n",
                "#Create time steps for Kalman and linear regression\n",
                "tsteps_hist = np.linspace(1, 30.436875*backcast_length, backcast_length)\n",
                "tsteps_monitoring = np.linspace(tsteps_hist[-1], 30.436875*forecast_length, forecast_length)\n",
                "\n",
                "# Model config.\n",
                "epochs = 10\n",
                "bach_size = 128\n",
                "output_dim = 1  # Keras n-beats supports multivariate modelling changing this param.\n",
                "\n",
                "#For initial testing on a single pixel\n",
                "pixel = 15000\n",
                "\n",
                "sweep_config = {\n",
                "    'method': 'bayes'\n",
                "    }\n",
                "\n",
                "metric = {\n",
                "    'name': 'loss',\n",
                "    'goal': 'minimize'   \n",
                "    }\n",
                "\n",
                "sweep_config['metric'] = metric\n",
                "\n",
                "param_grid = {'epochs':{'values':[10,20]},\n",
                "              'widths':{'values':[[32,256], [64,256], [32,512], [64,512], [32,1024],[64, 1024]]}, \n",
                "              'blocks':{'values':[[1,1],[2,2],[3,3]]},\n",
                "              'layers':{'values':[[2,2],[3,3],[4,4]]},\n",
                "              'batch_size':{'values':[128,256,512,1024]},\n",
                "              'weight_decay':{'values':[1e-2,1e-3]}, \n",
                "              'sharing':{'values':[True,False]}}\n",
                "sweep_config['parameters'] = param_grid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'wandb' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39msweep(sweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname of your wandb project\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
                    ]
                }
            ],
            "source": [
                "sweep_id = wandb.sweep(sweep_config, project=\"name of your wandb project\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing\n",
                "\n",
                "Use of the previously written functions and split data to replace nans, split into the different bands, interpolate and calculate indices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(2, 65393, 60)\n",
                        "(2, 16342, 60)\n",
                        "(2, 117474, 60)\n"
                    ]
                }
            ],
            "source": [
                "training_set = training_set\n",
                "validation_set = validation_set\n",
                "testing_set = testing_set\n",
                "\n",
                "data = time_series['32_N1139_W11'].values.compute()\n",
                "data = data\n",
                "\n",
                "\n",
                "# Replacing given NaN value with np.nan in the three sets\n",
                "training_set = np.where(training_set == nodata_value, np.nan, training_set)\n",
                "validation_set = np.where(validation_set == nodata_value, np.nan, validation_set)\n",
                "testing_set = np.where(testing_set == nodata_value, np.nan, testing_set)\n",
                "data = np.where(data == nodata_value, np.nan, data)\n",
                "\n",
                "\n",
                "training_set = np.where(training_set <= 0, np.nan, training_set)\n",
                "validation_set = np.where(validation_set <= 0, np.nan, validation_set)\n",
                "testing_set = np.where(testing_set <= 0, np.nan, testing_set)\n",
                "data = np.where(data <= 0, np.nan, data)\n",
                "\n",
                "\n",
                "training_set = average_time_series(training_set)\n",
                "validation_set = average_time_series(validation_set)\n",
                "testing_set = average_time_series(testing_set)\n",
                "data = average_time_series(data)\n",
                "\n",
                "\n",
                "training_set = np.array(training_set)\n",
                "validation_set = np.array(validation_set)\n",
                "testing_set = np.array(testing_set)\n",
                "data = np.array(data)\n",
                "\n",
                "\n",
                "training_set = da.moveaxis(training_set, 0, 1)\n",
                "validation_set = da.moveaxis(validation_set, 0, 1)\n",
                "testing_set = da.moveaxis(testing_set, 0, 1)\n",
                "data = da.moveaxis(data, 0, 1)\n",
                "\n",
                "\n",
                "training_set = training_set / data_scale\n",
                "validation_set = validation_set / data_scale\n",
                "testing_set = testing_set / data_scale\n",
                "data = data / data_scale\n",
                "\n",
                "\n",
                "training_set[:,:,56] = np.nan\n",
                "validation_set[:,:,56] = np.nan\n",
                "testing_set[:,:,56] = np.nan\n",
                "data[:,:,56] = np.nan\n",
                "\n",
                "\n",
                "\n",
                "red_training = training_set[0,:,:]\n",
                "nir_training = training_set[1,:,:]\n",
                "\n",
                "red_validation = validation_set[0,:,:]\n",
                "nir_validation = validation_set[1,:,:]\n",
                "\n",
                "red_testing = testing_set[0,:,:]\n",
                "nir_testing = testing_set[1,:,:]\n",
                "\n",
                "red = data[0,:,:]\n",
                "nir = data[1,:,:]\n",
                "\n",
                "# greenI_training  = training_set[0,:,:]\n",
                "# yellow_training = training_set[1,:,:]\n",
                "\n",
                "# greenI_validation = validation_set[0,:,:]\n",
                "# yellow_validation = validation_set[1,:,:]\n",
                "\n",
                "# greenI_testing = testing_set[0,:,:]\n",
                "# yellow_testing = testing_set[1,:,:]\n",
                "\n",
                "# greenI = data[0,:,:]\n",
                "# yellow = data[1,:,:]\n",
                "\n",
                "# red = data[0,:,:]\n",
                "# nir = data[1,:,:]\n",
                "\n",
                "\n",
                "print(training_set.shape)\n",
                "print(validation_set.shape)\n",
                "print(testing_set.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_interpolate(dataset,red_band, nir_band):\n",
                "    steps_red = []\n",
                "    steps_nir = []\n",
                "    \n",
                "    for i in tqdm(range(dataset.shape[1])):\n",
                "        try:\n",
                "            steps_red.append(interpolate_band(red_band[i,:]))\n",
                "            steps_nir.append(interpolate_band(nir_band[i,:]))\n",
                "        except ValueError:\n",
                "            steps_red.append(red_band[i,:])\n",
                "            steps_nir.append(nir_band[i,:])\n",
                "            \n",
                "    red_band = np.row_stack(steps_red)  \n",
                "\n",
                "    nir_band = np.row_stack(steps_nir)  \n",
                "\n",
                "    return red_band, nir_band"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 65393/65393 [00:01<00:00, 62561.21it/s]\n",
                        "100%|██████████| 16342/16342 [00:00<00:00, 64772.21it/s]\n",
                        "100%|██████████| 117474/117474 [00:01<00:00, 64309.35it/s]\n"
                    ]
                }
            ],
            "source": [
                "red_training,nir_training = run_interpolate(training_set,red_training,nir_training)\n",
                "red_validation,nir_validation = run_interpolate(validation_set,red_validation,nir_validation)\n",
                "red_testing,nir_testing = run_interpolate(testing_set,red_testing,nir_testing)\n",
                "\n",
                "# redge, nir = run_interpolate(data,redge,nir)\n",
                "\n",
                "# greenI_training,yellow_training = run_interpolate(training_set,greenI_training,yellow_training)\n",
                "# greenI_validation,yellow_validation = run_interpolate(validation_set,greenI_validation,yellow_validation)\n",
                "# greenI_testing,yellow_testing = run_interpolate(testing_set,greenI_testing,yellow_testing)\n",
                "\n",
                "# red,nir = run_interpolate(data,red,nir)\n",
                "\n",
                "labels = time_series.labels_\n",
                "labels = np.array(labels)\n",
                "ndvi_training  =  (nir_training - red_training) / (nir_training + red_training)\n",
                "ndvi_validation  =  (nir_validation - red_validation) / (nir_validation + red_validation)\n",
                "ndvi_testing =  (nir_testing - red_testing) / (nir_testing + red_testing)\n",
                "ndvi =  (nir - red) / (nir + red)\n",
                "\n",
                "# ndre_training  =  (nir_training - redge_training) / (nir_training + redge_training)\n",
                "# ndre_validation  =  (nir_validation - redge_validation) / (nir_validation + redge_validation)\n",
                "# ndre_testing =  (nir_testing - redge_testing) / (nir_testing + redge_testing)\n",
                "# ndre =  (nir - redge) / (nir + redge)\n",
                "\n",
                "# training_set = yellow_training\n",
                "# validation_set = yellow_validation\n",
                "# testing_set = yellow_testing\n",
                "\n",
                "# training_set = greenI_training\n",
                "# validation_set = greenI_validation\n",
                "# testing_set = greenI_testing\n",
                "\n",
                "training_set = ndvi_training\n",
                "validation_set = ndvi_validation\n",
                "testing_set = ndvi_testing\n",
                "\n",
                "# data = ndvi\n",
                "\n",
                "# training_set = training_set[training_slice,:]\n",
                "# validation_set = validation_set[validation_slice,:]\n",
                "# testing_set = testing_set[testing_slice,:]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Function to create GeoDataframes for NBEATS Forecasting training and testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_datasets(training_array,validation_array,testing_array,dates):    \n",
                "    training_dataset = pd.DataFrame(training_array)\n",
                "    training_dataset = training_dataset.transpose()\n",
                "    # training_dataset = training_dataset.apply(lambda x: hampel(x,n_sigma=2.0).filtered_data)\n",
                "    training_dataset['dates'] = dates\n",
                "    training_dataset.set_index('dates',append=True,inplace=True)\n",
                "    training_dataset = training_dataset.transpose()\n",
                "    training_dataset = training_dataset.stack(list(range(training_dataset.columns.nlevels)))\n",
                "    training_dataset = pd.DataFrame(training_dataset)\n",
                "    training_dataset= training_dataset.rename(columns = {0:'values'})\n",
                "    training_dataset.index.names = ['group','time_idx','dates']\n",
                "    training_dataset = training_dataset.reset_index()\n",
                "\n",
                "    validation_dataset = pd.DataFrame(validation_array)\n",
                "    validation_dataset = validation_dataset.transpose()\n",
                "    # validation_dataset = validation_dataset.apply(lambda x: hampel(x,n_sigma=2.0).filtered_data)\n",
                "    validation_dataset['dates'] = dates\n",
                "    validation_dataset.set_index('dates',append=True,inplace=True)\n",
                "    validation_dataset = validation_dataset.transpose()\n",
                "    validation_dataset = validation_dataset.stack(list(range(validation_dataset.columns.nlevels)))\n",
                "    validation_dataset = pd.DataFrame(validation_dataset)\n",
                "    validation_dataset= validation_dataset.rename(columns = {0:'values'})\n",
                "    validation_dataset.index.names = ['group','time_idx','dates']\n",
                "    validation_dataset = validation_dataset.reset_index()\n",
                "\n",
                "    testing_dataset = pd.DataFrame(testing_array)\n",
                "    testing_dataset = testing_dataset.transpose()\n",
                "    # testing_dataset = testing_dataset.apply(lambda x: hampel(x,n_sigma=2.0).filtered_data)\n",
                "    # dataset_testing = testing_hampel.filtered_data\n",
                "    testing_dataset['dates'] = dates\n",
                "    testing_dataset.set_index('dates',append=True,inplace=True)\n",
                "    testing_dataset = testing_dataset.transpose()\n",
                "    testing_dataset = testing_dataset.stack(list(range(testing_dataset.columns.nlevels)))\n",
                "    testing_dataset = pd.DataFrame(testing_dataset)\n",
                "    testing_dataset = testing_dataset.rename(columns = {0:'values'})\n",
                "    testing_dataset.index.names = ['group','time_idx','dates']\n",
                "    testing_dataset = testing_dataset.reset_index()\n",
                "\n",
                "    return training_dataset,validation_dataset,testing_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Function to train model and do hyperparameter tuning with wandb\n",
                "\n",
                "Command to perform sweep, and then save the best metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def train_fit_model(config=None):\n",
                "     \n",
                "#     training_dataset,validation_dataset,testing_dataset = create_datasets(training_set,validation_set,testing_set,dates[3:])\n",
                "#     # create dataset and dataloaders\n",
                "#     max_encoder_length = backcast_length\n",
                "#     max_prediction_length = forecast_length\n",
                "\n",
                "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "\n",
                "#     training_cutoff = training_dataset[\"time_idx\"].max() - max_prediction_length\n",
                "\n",
                "#     context_length = max_encoder_length\n",
                "#     prediction_length = max_prediction_length\n",
                "\n",
                "\n",
                "#     training = TimeSeriesDataSet(\n",
                "#     training_dataset,\n",
                "#     time_idx=\"time_idx\",\n",
                "#     target=\"values\",\n",
                "#     categorical_encoders={\"group\": NaNLabelEncoder().fit(training_dataset.group)},\n",
                "#     group_ids=[\"group\"],\n",
                "#     # only unknown variable is \"value\" - and N-Beats can also not take any additional variables\n",
                "#     time_varying_unknown_reals=[\"values\"],\n",
                "#     max_encoder_length=context_length,\n",
                "#     max_prediction_length=prediction_length,\n",
                "#     allow_missing_timesteps=True\n",
                "# )\n",
                "    \n",
                "#     # checkpoint_callback = ModelCheckpoint(monitor='val_accuracy', mode='max')\n",
                "\n",
                "#     with wandb.init(config=config):\n",
                "#         wandb_logger = WandbLogger(project='nbeats-wand-forecasting', log_model='all')\n",
                "#         config = wandb.config\n",
                "\n",
                "#         train_dataloader = training.to_dataloader(train=True, batch_size=config.batch_size, num_workers=0)\n",
                "#         validation = TimeSeriesDataSet.from_dataset(training, validation_dataset, min_prediction_idx=training_cutoff + 1)\n",
                "#         val_dataloader = validation.to_dataloader(train=False, batch_size=config.batch_size, num_workers=0)\n",
                "\n",
                "        \n",
                "#         # calculate baseline absolute error\n",
                "#         actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).cuda()\n",
                "#         baseline_predictions = Baseline().predict(val_dataloader).cuda()\n",
                "#         SMAPE()(baseline_predictions, actuals)\n",
                "       \n",
                "#         baseline = SMAPE()(baseline_predictions, actuals)\n",
                "#         wandb.log({\"Baseline SMAPE\": baseline})\n",
                "\n",
                "#         pl.seed_everything(42)\n",
                "#         trainer = pl.Trainer(accelerator=\"gpu\", gradient_clip_val=0.01,devices=1,logger=wandb_logger)\n",
                "#         net = NBeats.from_dataset(training, learning_rate=3e-2, weight_decay=config.weight_decay, widths=config.widths, backcast_loss_ratio=0.1)\n",
                "\n",
                "#         # find optimal learning rate\n",
                "#         res = Tuner(trainer).lr_find(net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5)\n",
                "#         print(f\"suggested learning rate: {res.suggestion()}\")\n",
                "#         # fig = res.plot(show=True, suggest=True)\n",
                "#         # fig.show()\n",
                "#         net.hparams.learning_rate = res.suggestion()\n",
                "#         lr_suggested = net.hparams.learning_rate\n",
                "\n",
                "#         print(lr_suggested)\n",
                "#         #Fitting Model\n",
                "#         early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
                "#         trainer = pl.Trainer(\n",
                "#             max_epochs=config.epochs,\n",
                "#             accelerator=\"gpu\",\n",
                "#             enable_model_summary=True,\n",
                "#             gradient_clip_val=0.01,\n",
                "#             callbacks=[early_stop_callback],\n",
                "#             limit_train_batches=150,\n",
                "#             logger=wandb_logger\n",
                "#         )\n",
                "        \n",
                "        \n",
                "\n",
                "#         net = NBeats.from_dataset(\n",
                "#             training,\n",
                "#             learning_rate=lr_suggested,\n",
                "#             num_blocks = config.blocks,\n",
                "#             num_block_layers =config.layers,\n",
                "#             # log_interval=10,\n",
                "#             # log_val_interval=1,\n",
                "#             weight_decay=config.weight_decay,\n",
                "#             widths=config.widths,\n",
                "#             sharing = config.sharing,\n",
                "#             backcast_loss_ratio=1.0,\n",
                "            \n",
                "#         ) \n",
                "        \n",
                "#         net.to(device)\n",
                "        \n",
                "#         # wandb_logger.watch(net, log_graph=False)\n",
                "\n",
                "#         trainer.fit(\n",
                "#             net,\n",
                "#             train_dataloaders=train_dataloader,\n",
                "#             val_dataloaders=val_dataloader,\n",
                "           \n",
                "#         )\n",
                "        \n",
                "\n",
                "#         #Validation error calculation\n",
                "#         best_model_path = trainer.checkpoint_callback.best_model_path\n",
                "#         best_model = NBeats.load_from_checkpoint(best_model_path)\n",
                "\n",
                "#         actuals_validation = torch.cat([y[0] for x, y in iter(val_dataloader)]).cuda()\n",
                "#         predictions_validation = best_model.predict(val_dataloader).cuda()\n",
                "        \n",
                "#         smape_validation = SMAPE()(predictions_validation,actuals_validation).detach().cpu().numpy()\n",
                "#         rmse_validation = RMSE()(predictions_validation,actuals_validation).detach().cpu().numpy()\n",
                "\n",
                "#         error_validation = (actuals_validation - predictions_validation).abs().mean()\n",
                "\n",
                "#         #Predictions\n",
                "#         testing = TimeSeriesDataSet.from_dataset(training, testing_dataset, min_prediction_idx=training_cutoff + 1)\n",
                "#         test_dataloader = testing.to_dataloader(train=False, batch_size=config.batch_size, num_workers=0)\n",
                "\n",
                "\n",
                "#         actuals = torch.cat([y[0] for x, y in iter(test_dataloader)]).cuda()\n",
                "#         predictions = best_model.predict(test_dataloader).cuda()\n",
                "       \n",
                "#         smape = SMAPE()(predictions,actuals).detach().cpu().numpy()\n",
                "#         rmse = RMSE()(predictions,actuals).detach().cpu().numpy()\n",
                "#         # (actuals - predictions).abs().mean()\n",
                "       \n",
                "#         items = best_model.predict(test_dataloader, mode='raw', return_x=True)\n",
                "#         x = items.x\n",
                "#         raw_predictions = items[0]\n",
                "#         nb_predictions_testing = raw_predictions.prediction.detach().cpu().numpy()\n",
                "        \n",
                "    \n",
                "#         wandb.log({ \"SMAPE\": smape, 'RMSE':rmse})\n",
                "\n",
                "#         return nb_predictions_testing\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# wandb.agent(sweep_id, train_fit_model, count=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "\n",
                "# run is specified by <entity>/<project>/<run_id>\n",
                "run = api.run(\"vegetation-team/nbeats-wand-forecasting/2jknx9e0\")\n",
                "\n",
                "# save the metrics for the run to a csv file\n",
                "metrics_dataframe = run.history()\n",
                "metrics_dataframe.to_csv(\"/home/ubuntu/liveeo_modules/pablo_lab/pablo_lab/Results/wandb_boren_ndvi_metrics.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cells to train single model on the best hyperparameters found through optimization\n",
                "\n",
                "Followed by cell to fit model on the testing tile to save the predictions and the model parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-13 11:14:05.992689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2024-02-13 11:14:06.727881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "Seed set to 42\n",
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a0e0d4a7cfca4dcd9ccb9d58548ab15e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_steps=100` reached.\n",
                        "Learning rate set to 0.05011872336272722\n",
                        "Restoring states from the checkpoint path at /home/ubuntu/liveeo_modules/pablo_lab/pablo_lab/.lr_find_84b4a8a3-e3ab-411d-92fd-bdf0908419ba.ckpt\n",
                        "Restored all states from the checkpoint at /home/ubuntu/liveeo_modules/pablo_lab/pablo_lab/.lr_find_84b4a8a3-e3ab-411d-92fd-bdf0908419ba.ckpt\n",
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "\n",
                        "  | Name            | Type       | Params\n",
                        "-----------------------------------------------\n",
                        "0 | loss            | MASE       | 0     \n",
                        "1 | logging_metrics | ModuleList | 0     \n",
                        "2 | net_blocks      | ModuleList | 473 K \n",
                        "-----------------------------------------------\n",
                        "473 K     Trainable params\n",
                        "0         Non-trainable params\n",
                        "473 K     Total params\n",
                        "1.892     Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "suggested learning rate: 0.05011872336272722\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c6becade277a49d5b32974981a72163d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "69c75f2564294773bbbf1c6b357c12e2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "42ccef7591044d97b6188235de71932c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f550bf3cc97e481cbf9b27227a92ff2b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3fe6f88d1b3d4f0ba9269cbbfc8bd581",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "07d00dc95dfc457ba8730837946c8e8c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6e60612cbc2f413ca1cbeac9bfe33b8b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0253e99b55b544ee94299ed1458e8d41",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "985e86fefbb04984b166b30d47b3170c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "90293cb3dd90489d89f34f9623b2b470",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "544dc5b2ca0e461db1337ffda666a16c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e6f7014a5b084c43b7136058fa760ca0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "04e3b60954874b798dad8e67dd8c947b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1aeac4f9faec452dbd6406f214e8f2cd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "63dbb8f1ec164962a85392c167a9fb75",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "68a17d50126147cda296c8c97bd3eb9e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3a61c8054e894e20acb8e9ed72d883d0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5a6ad8255e9145e49fdddd5f3fae2431",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d884f40e3334497b9adc99bc829f00fd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "477012093fbc4ac7a779a2ad34713240",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "42ff057acc8445b09c289bf163556fbd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d65c06c6fd3d4799bc58ee75c920a443",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
                    ]
                }
            ],
            "source": [
                "# create dataset and dataloaders\n",
                "training_dataset,validation_dataset,testing_dataset = create_datasets(training_set,validation_set,testing_set,dates)\n",
                "max_encoder_length = backcast_length\n",
                "max_prediction_length = forecast_length\n",
                "\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "\n",
                "training_cutoff = training_dataset[\"time_idx\"].max() - max_prediction_length\n",
                "\n",
                "context_length = max_encoder_length\n",
                "prediction_length = max_prediction_length\n",
                "\n",
                "\n",
                "training = TimeSeriesDataSet(\n",
                "training_dataset,\n",
                "time_idx=\"time_idx\",\n",
                "target=\"values\",\n",
                "categorical_encoders={\"group\": NaNLabelEncoder().fit(training_dataset.group)},\n",
                "group_ids=[\"group\"],\n",
                "# only unknown variable is \"value\" - and N-Beats can also not take any additional variables\n",
                "time_varying_unknown_reals=[\"values\"],\n",
                "max_encoder_length=context_length,\n",
                "max_prediction_length=prediction_length,\n",
                "allow_missing_timesteps=True\n",
                ")\n",
                "\n",
                "\n",
                "train_dataloader = training.to_dataloader(train=True, batch_size=512, num_workers=0)\n",
                "validation = TimeSeriesDataSet.from_dataset(training, validation_dataset, min_prediction_idx=training_cutoff + 1)\n",
                "val_dataloader = validation.to_dataloader(train=False, batch_size=512, num_workers=0)\n",
                "\n",
                "# calculate baseline absolute error\n",
                "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).cuda()\n",
                "baseline_predictions = Baseline().predict(val_dataloader).cuda()\n",
                "SMAPE()(baseline_predictions, actuals)\n",
                "\n",
                "pl.seed_everything(42)\n",
                "trainer = pl.Trainer(accelerator=\"auto\", gradient_clip_val=0.01,devices=1)\n",
                "net = NBeats.from_dataset(training, learning_rate=3e-2, weight_decay=1e-2, widths=[32, 512], backcast_loss_ratio=0.1)\n",
                "\n",
                "# find optimal learning rate\n",
                "res = Tuner(trainer).lr_find(net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5)\n",
                "print(f\"suggested learning rate: {res.suggestion()}\")\n",
                "# fig = res.plot(show=True, suggest=True)\n",
                "# fig.show()\n",
                "net.hparams.learning_rate = res.suggestion()\n",
                "lr_suggested = net.hparams.learning_rate\n",
                "\n",
                "\n",
                "#Fitting Model\n",
                "early_stop_callback = EarlyStopping(monitor=\"val_SMAPE\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=20,\n",
                "    accelerator=\"auto\",\n",
                "    enable_model_summary=True,\n",
                "    gradient_clip_val=0.01,\n",
                "    callbacks=[early_stop_callback],\n",
                "    limit_train_batches=150\n",
                ")\n",
                "\n",
                "net = NBeats.from_dataset(\n",
                "    training,\n",
                "    learning_rate=lr_suggested,\n",
                "    # num_blocks = [3],\n",
                "    # num_block_layers = [3],\n",
                "    log_interval=-1,\n",
                "    log_val_interval=1,\n",
                "    weight_decay=0.01,\n",
                "    widths=[64,256],\n",
                "    sharing = False,\n",
                "    backcast_loss_ratio=1.0\n",
                ") \n",
                "\n",
                "net.to(device)\n",
                "\n",
                "trainer.fit(\n",
                "    net,\n",
                "    train_dataloaders=train_dataloader,\n",
                "    val_dataloaders=val_dataloader,\n",
                ")\n",
                "\n",
                "#Predictions\n",
                "testing = TimeSeriesDataSet.from_dataset(training, testing_dataset, min_prediction_idx=training_cutoff + 1)\n",
                "test_dataloader = testing.to_dataloader(train=False, batch_size=512, num_workers=0)\n",
                "\n",
                "\n",
                "best_model_path = trainer.checkpoint_callback.best_model_path\n",
                "best_model = NBeats.load_from_checkpoint(best_model_path)\n",
                "\n",
                "actuals = torch.cat([y[0] for x, y in iter(test_dataloader)]).cuda()\n",
                "predictions = best_model.predict(test_dataloader).cuda()\n",
                "\n",
                "smape = SMAPE()(predictions,actuals)\n",
                "\n",
                "rmse = RMSE()(predictions,actuals)\n",
                "error_testing =  (actuals - predictions).abs().mean()\n",
                "\n",
                "items = best_model.predict(test_dataloader, mode='raw', return_x=True)\n",
                "x = items.x\n",
                "raw_predictions = items[0]\n",
                "nb_predictions_boren = raw_predictions.prediction.detach().cpu().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# #Fitting Model\n",
                "# early_stop_callback = EarlyStopping(monitor=\"val_SMAPE\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
                "# trainer = pl.Trainer(\n",
                "#     max_epochs=20,\n",
                "#     accelerator=\"gpu\",\n",
                "#     enable_model_summary=True,\n",
                "#     gradient_clip_val=0.01,\n",
                "#     callbacks=[early_stop_callback],\n",
                "#     limit_train_batches=150\n",
                "# )\n",
                "\n",
                "# net = NBeats.from_dataset(\n",
                "#     training,\n",
                "#     learning_rate=lr_suggested,\n",
                "#     # num_blocks = [3],\n",
                "#     # num_block_layers = [3],\n",
                "#     log_interval=-1,\n",
                "#     log_val_interval=1,\n",
                "#     weight_decay=0.001,\n",
                "#     widths=[32,1024],\n",
                "#     sharing = False,\n",
                "#     backcast_loss_ratio=1.0\n",
                "# ) \n",
                "\n",
                "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
                "# best_model = NBeats.load_from_checkpoint(best_model_path)\n",
                "\n",
                "# testing_32_N1139_W11 = nbt.create_single_dataset(ndre,dates[3:])\n",
                "# testing = TimeSeriesDataSet.from_dataset(training, testing_dataset, min_prediction_idx=training_cutoff + 1)\n",
                "# test_dataloader = testing.to_dataloader(train=False, batch_size=1024, num_workers=0)\n",
                "\n",
                "\n",
                "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
                "# best_model = NBeats.load_from_checkpoint(best_model_path)\n",
                "\n",
                "# actuals = torch.cat([y[0] for x, y in iter(test_dataloader)]).cuda()\n",
                "# predictions = best_model.predict(test_dataloader).cuda()\n",
                "\n",
                "# smape = SMAPE()(predictions,actuals)\n",
                "\n",
                "# rmse = RMSE()(predictions,actuals)\n",
                "# error_testing =  (actuals - predictions).abs().mean()\n",
                "\n",
                "# items = best_model.predict(test_dataloader, mode='raw', return_x=True)\n",
                "# x = items.x\n",
                "# raw_predictions = items[0]\n",
                "# nb_predictions_redge_boren = raw_predictions.prediction.detach().cpu().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(best_model.state_dict(),'/home/ubuntu/liveeo_modules/pablo_lab/pablo_lab/Results/nbeats_boren_ndvi_model.pt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "nbeats_predictions = pd.DataFrame(nb_predictions_boren)\n",
                "nbeats_predictions.to_csv('/home/ubuntu/liveeo_modules/pablo_lab/pablo_lab/Results/nbeats_predictions_ndvi_testing.csv')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pablo_lab",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
